#+title: Chapter 3 - Essential Linear Algebra
#+date: <2025-08-10 Sun 08:06>
#+startup: latexpreview
#+startup: overview

* TODO Exercises [21/32]
:PROPERTIES:
:COOKIE_DATA: todo recursive
:END:
** DONE 3.1 [7/7]
*** DONE 1.
CLOSED: [2025-08-11 Mon 07:36]


\begin{equation*}
f(x) = ax + b
\end{equation*}

Definition 3.9 states

Suppose $X$ and $Y$ are vector spaces

$f:X \to Y$

$f$ *linear* if and only if


\begin{equation*}
f(\alpha\mathbf{x} + \beta\mathbf{z}) = \alpha f(\mathbf{x}) + \beta f(\mathbf{z})
\end{equation*}

for all $\alpha, \beta \in \mathbf{R}$, $\mathbf{x}, \mathbf{z} \in X$.

\begin{equation*}
\begin{align}
f(\alpha x + \beta z) & = a(\alpha x + \beta z) + b \\
& = \alpha a x + \beta a z + b \\
& \ne f(\alpha x) + f(\alpha z)
\end{align}
\end{equation*}

By Definition 3.9, $f$ is *not* linear.

The form of a linear function $f : \mathbf{R} \to \mathbf{R}$ is $f(x) = ax$.

\begin{equation*}
\begin{align}
f(\alpha x + \beta z) & = a(\alpha x + \beta z) \\
& = \alpha a x + \beta a z \\
& = \alpha f(x) + \beta f(z)
\end{align}
\end{equation*}

*** DONE 3.
CLOSED: [2025-08-11 Mon 19:30]


\begin{equation*}
f(\mathbf{x}) = \begin{bmatrix}
x_1^2 + x_2^2 \\
x_2 - x_1^2
\end{bmatrix}
\end{equation*}

Prove that $f$ is not linear.

\begin{equation*}
f(\alpha\mathbf{x}) = \begin{bmatrix}
    \alpha^2 x_1^2 + \alpha^2 x_2^2 \\
    \alpha x_2 - \alpha^2 x_1^2
\end{bmatrix}
\end{equation*}


\begin{equation*}
\alpha f(\mathbf{x}) = \begin{bmatrix}
    \alpha x_1^2 + \alpha x_2^2 \\
    \alpha x_2 - \alpha x_1^2
\end{bmatrix}
\end{equation*}

$f(\alpha \mathbf{x}) \ne \alpha f(\mathbf{x})$

$f(\mathbf{x})$ is not linear.

*** DONE 5.
CLOSED: [2025-08-12 Tue 18:04]

**** (a) $\lbrace f \in C[0,1] : f(0) = 0 \rbrace$

Let $W = \lbrace f \in C[0,1] : f(0) = 0 \rbrace$.

- The zero function is in $W$.

- Let $f,g\in W$, $w = \alpha f + \beta g \in W$ since a continuous function added to a continuous function is a continuous function.

- $w(0) = \alpha f(0) + \beta g(0) = 0$.

*$W$ is a vector space (subspace).*

**** (b) $W = \lbrace f \in C[0,1] : f(0) = 1\rbrace$

- The zero function is not in $W$.

*$W$ is not a vector space.*

**** (c) $W = \lbrace f \in C[0,1] : \int_0^1 f(x)dx = 0 \rbrace$

- The zero function is in $W$.

- Let $f,g\in W$, $\alpha,\beta \in \mathbf{R}$, and $w = \alpha f + \beta g$.

  \begin{equation*}
  \begin{align}
  \int^1_0 w(x) & = \int^1_0 \left[\alpha f(x) + \beta g(x)\right] dx \\
  & = \alpha \int^1_0 f(x) dx + \beta g(x) dx \\
  & = \alpha 0 + \beta 0 \\
  & = 0
  \end{align}
  \end{equation*}

  Therefore, $w \in W$.

*$W$ is a vector space.*

**** (d) $\mathcal{P}_n$, the set of all polynomials of degree n or less.

- The zero polynomial is in $\mathcal{P}_n$.

- $r = \alpha p + \beta q$, $r$ is in $\mathcal{P}_n$.

*$\mathcal{P}_n$ is a vector space.*

**** (e) The set of all polynomials of degree exactly $n$.

The zero polynomial isn't in the set.

The set is not a vector space.

*** DONE 7.
CLOSED: [2025-08-12 Tue 18:23]


\begin{equation*}
Lu = \frac{du}{dx} + u^3
\end{equation*}

\begin{equation*}
L(\alpha u + \beta v) = \frac{d}{dx}(\alpha u + \beta v) + (\alpha u + \beta v)^3
\end{equation*}


\begin{equation*}
\begin{align}
(\alpha u + \beta v)^3 & = (\alpha^2u^2 + 2\alpha\beta u v + \beta^2v^2)(\alpha u + \beta v) \\
& = \alpha^3 u^3
    + 3\alpha^2\beta u^2 v + 3\alpha\beta^2 u v^2
    + \beta^3 v^3
\end{align}
\end{equation*}

\begin{equation*}
\begin{align}
\alpha L(u) + \beta L(v) = \alpha\frac{du}{dx} + \alpha u^3
    + \beta\frac{dv}{dx} + \beta v^3
\end{align}
\end{equation*}


\begin{equation*}
L(\alpha u + \beta v) \ne \alpha L(u) + \beta L(v)
\end{equation*}

$L$ is not linear.

*** DONE 9.
CLOSED: [2025-08-13 Wed 21:18]



\begin{equation*}
Ku : C^2[a,b] \to C[a,b]
\end{equation*}


\begin{equation*}
Ku = x^2\frac{d^2 u}{dx^2} - 2x\frac{du}{dx} + 3u
\end{equation*}

\begin{equation*}
\begin{align}
K\alpha u & = x^2\frac{d^2 \alpha u}{dx^2} - 2x\frac{d \alpha u}{dx} + 3(\alpha u) \\
& = \alpha x^2\frac{d^2 u}{dx^2} - \alpha\left(2x\frac{du}{dx}\right) + \alpha(3u) \\
& = \alpha \left(x^2\frac{d^2x}{dx^2} - 2\frac{du}{dx} + 3u\right) \\
& = \alpha Ku
\end{align}
\end{equation*}

\begin{equation*}
\begin{align}
K(u + v) & = x^2\frac{d^2}{dx^2}(u + v) - 2x\frac{d}{dx}(u+v) + 3(u+v) \\
& = x^2\frac{d^2 u}{dx^2} - 2x\frac{du}{dx} + 3u + x^2\frac{d^2 v}{dx^2} - 2x\frac{dv}{dx} + 3v \\
& = Ku + Kv
\end{align}
\end{equation*}

$K$ is linear.

*** DONE 11.
CLOSED: [2025-08-13 Wed 21:41]

Let $\rho$, $c$, and $\kappa$ be constants, and $L : C^2(\mathbf{R}^2) \to C(\mathbf{R^2})$.


\begin{equation*}
Lu = \rho c \frac{\partial u}{\partial t} - \kappa \frac{\partial ^2 u}{\partial x^2}
\end{equation*}

Prove that $L$ is a linear operator.

\begin{equation*}
\begin{align}
L\alpha u & = \rho c \frac{\partial\alpha u}{\partial t} - \kappa\frac{\partial^2\alpha u}{\partial x^2} \\
& = \alpha\left(\rho c\frac{\partial^2 u}{\partial t} - \kappa\frac{\partial^2 u}{\partial x^2}\right) \\
& = \alpha Lu
\end{align}
\end{equation*}


\begin{equation*}
\begin{align}
L(u + v) & = \rho c\frac{\partial}{\partial t}(u + v) - \kappa\frac{\partial^2}{\partial x^2}(u + v) \\
& = \rho c \frac{\partial u}{\partial t} - \kappa \frac{\partial^2 u}{\partial x^2} + \rho c \frac{\partial v}{\partial t} - \kappa \frac{\partial^2 v}{\partial x^2}
& = Lu + Lv
\end{align}
\end{equation*}

$L$ is linear.

*** DONE 13.
CLOSED: [2025-08-14 Thu 19:27]

**** (a)

Let $\mathbf{A} \in \mathbf{R}^{2\times 2}$. Prove that


\begin{equation*}
\mathbf{A}(\alpha\mathbf{x} + \beta\mathbf{y}) = \alpha\mathbf{Ax} + \beta\mathbf{Ay}
\end{equation*}

for all $\alpha,\beta\,\in\mathbf{R}$, $\mathbf{x},\mathbf{y}\in\mathbf{R}^2$.

Let

\begin{equation*}
\mathbf{A} = \begin{bmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{bmatrix}, \:
\mathbf{x} = \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}, \:
\mathbf{y} = \begin{bmatrix} y_1 \\ y_2 \end{bmatrix}.
\end{equation*}



\begin{equation*}
\begin{align}
\mathbf{A}(\alpha\mathbf{x} + \beta\mathbf{y}) & =
\begin{bmatrix}a_{11} & a_{12} \\ a_{21} & a_{22}\end{bmatrix}\begin{bmatrix}\alpha x_1 + \beta y_1 \\ \alpha x_2 + \beta y_2\end{bmatrix} \\
& = \begin{bmatrix}
a_{11}(\alpha x_1 + \beta y_1) + a_{12}(\alpha x_2 + \beta y_2) \\
a_{21}(\alpha x_1 + \beta y_1) + a_{22}(\alpha x_2 + \beta y_2)
\end{bmatrix} \\
& = \begin{bmatrix}
\alpha a_{11}x_1 + \alpha a_{12}x_2 + \beta a_{11}y_1 + \beta a_{12}y_2 \\
\alpha a_{21}x_1 + \alpha a_{22}x_2 + \beta a_{21}y_1 + \beta a_{22}y_2
\end{bmatrix} \\
& = \alpha\begin{bmatrix}
a_{11}x_1 + a_{12}x_2 \\
a_{21}x_1 + a_{22}x_2
\end{bmatrix} + \beta\begin{bmatrix}
a_{11}y_1 + a_{12}y_2 \\
a_{21}y_1 + a_{22}y_2
\end{bmatrix} \\
& = \alpha\mathbf{A}\mathbf{x} + \beta\mathbf{A}\mathbf{y}
\end{align}
\end{equation*}

$\therefore$

\begin{equation*}
\mathbf{A}(\alpha\mathbf{x} + \beta\mathbf{y}) = \alpha\mathbf{A}\mathbf{x} + \beta\mathbf{A}\mathbf{y}
\end{equation*}

**** (b)

Now repeat part 13(a) for $\mathbf{A}\in\mathbf{R}^{n\times n}$.

\begin{equation*}
(\mathbf{A}\mathbf{x})_i=\sum^n_{j=1} a_{ij}x_j
\end{equation*}

 \begin{equation*}
\begin{align}
(\mathbf{A}(\alpha\mathbf{x} + \beta\mathbf{y}))_i & = \sum_{j=1}^na_{ij}(\alpha x_j + \beta y_j) \\
& = \sum_{j=1}^n\alpha a_{ij}x_j + \beta a_{ij}y_j \\
& = \alpha\sum_{j=1}^n a_{ij}x_j + \beta\sum_{j=1}^n a_{ij}y_j \\
& = (\alpha\mathbf{A}\mathbf{x})_i + (\beta\mathbf{A}\mathbf{y})_i
\end{align}
\end{equation*}

** DONE 3.2 [6/6]
*** DONE 1.
CLOSED: [2025-08-17 Sun 17:15]

\begin{equation*}
\mathbf{A} =
\begin{bmatrix}
1 & 2 \\
-1 & -2
\end{bmatrix}
\end{equation*}

Graph $\mathcal{R}(\mathbf{A})$ in the plane.



\begin{equation*}
\mathbf{Ax} = \mathbf{b}
\end{equation*}

\begin{equation*}
\begin{align}
x_1 + 2x_2 & = b_1 \\
0 & = b_2 + b_1
\end{align}
\end{equation*}

For consistency, $b_2 = -b_1$

\begin{equation*}
x_1 = b_1 - 2x_2\\
\end{equation*}

Set $s = x_2$

\begin{equation*}
\mathbf{x} = \begin{bmatrix} b_1 \\ 0 \end{bmatrix} +
s \begin{bmatrix} -2 \\ 1 \end{bmatrix}
\end{equation*}

\begin{equation*}
\mathcal{N}(\mathbf{A}) = \begin{Bmatrix} s \begin{bmatrix} -2 \\ 1 \end{bmatrix} : s \in \mathbf{R} \end{Bmatrix}
\end{equation*}

\begin{equation*}
\mathcal{R}(\mathbf{A}) = \begin{Bmatrix} \begin{bmatrix} b_1 \\ b_2 \end{bmatrix} \in \mathbf{R}^2 : b_2 = -b_1 \end{Bmatrix}
\end{equation*}

#+begin_src gnuplot :file range.png :term png
  set title "Range of A"
  set xlabel "b1"
  set ylabel "b2"
  set zeroaxis
  set grid
  plot -x title "b2 = 2 b1" lw 2 lc rgb "blue"
#+end_src

#+RESULTS:
[[file:range.png]]

*** DONE 3.
CLOSED: [2025-08-17 Sun 21:08]

Determine if $\mathbf{Ax} = \mathbf{b}$ has a unique solution for each $\mathbf{b}$ (e.g. $\mathbf{A}$ is nonsingular).

**** (a)

\begin{equation*}
\mathbf{A} = \begin{bmatrix} 1 & -1 \\ -2 & 4 \end{bmatrix}
\end{equation*}

II - (-2) I

\begin{equation*} \begin{bmatrix} 1 & -1 \\ 0 & 1 \end{bmatrix} \end{equation*}

$\mathbf{A}$ is nonsingular.

**** (b)


\begin{equation*}
\mathbf{A} = \begin{bmatrix}1 & 2 \\ 3 & 4 \end{bmatrix}
\end{equation*}

II - 3I


\begin{equation*}
\begin{bmatrix}1 & 2 \\ 0 & -2\end{bmatrix}
\end{equation*}

$\mathbf{A}$ is nonsingular.

**** (c)

\\begin{equation*}
\mathbf{A} = \begin{bmatrix}1 & 1 \\ 1 & 1\end{bmatrix}
\end{equation*}

$\mathbf{A}$ is singular.


\begin{equation*}
\begin{align}
x_1 & = b_1 - x_2\\
0 & = b_2 - b_1
\end{align}
\end{equation*}

Solutions only exist when


\begin{equation*}
b_2 = b_1
\end{equation*}

So, when


\begin{equation*}
\begin{align}
\mathbf{x} & = \begin{bmatrix}1/2 \\ 1/2\end{bmatrix}, \\
\mathbf{b} & = \begin{bmatrix}1 \\ 1 \end{bmatrix}.
\end{align}
\end{equation*}

However, there is no solution $\mathbf{x}$ for

\begin{equation*}
\mathbf{b} = \begin{bmatrix}1 \\ 2\end{bmatrix}.
\end{equation*}

*** DONE 5.
CLOSED: [2025-08-19 Tue 07:31]

$\mathbf{A} \in \mathbf{R}^{n\times n}$

$\mathbf{b} \in \mathbf{R}^n,\,\mathbf{b}\ne 0$

Is the solution set of the equation $\mathbf{Ax} = \mathbf{b}$,


\begin{equation*}
\lbrace\mathbf{x}\in\mathbf{R}^n : \mathbf{Ax}=\mathbf{b}\rbrace,
\end{equation*}

a subspace of $\mathbf{R}^n$?

- Is $0$ in the solution set?
  If $\mathbf{Ax} = 0$, then $\mathbf{x} \in \mathcal{N}(\mathbf{A})$. The solution set excludes $\mathbf{x}$ for $\mathbf{b} = 0$.
  $\mathbf{x}=0$ is not in the solution set.

*The solution set is not a subspace of $\mathbf{R}^n$.*

*** DONE 7.
CLOSED: [2025-08-19 Tue 18:46]

\begin{equation*}
L : C^2[a,b]\to C[a,b]
\end{equation*}


\begin{equation*}
Lu = \frac{d^2u}{dx^2}
\end{equation*}

The null space of $L$, $\mathcal{N}(L)$, is the set of first-degree polynomials.

*** DONE 9.
CLOSED: [2025-08-19 Tue 19:53]

Define the set


\begin{equation*}
C^2_{\tilde{m}}[a,b] = \left\lbrace u\in C^2[a,b] : \frac{du}{dx}(a) = u(b) = 0  \right\rbrace
\end{equation*}

and $L_{\tilde{m}} : C^2_{\tilde{m}}[a,b] \to C[a,b]$, $L_{\tilde{m}} = -\frac{d^2u}{dx^2}$.

**** (a)

Determine the null space of $L_{\tilde{m}}$

\begin{equation*}
\begin{align}
\frac{d^2u}{dx^2} & = 0,\: a < x < b, \\
\frac{du}{dx}(a) & = 0, \\
u(b) & = 0
\end{align}
\end{equation*}

Because of the operator,   $u(x) = C_1x + C_2$

\begin{equation*}
\begin{align}
& \frac{du}{dx}(a) = C_1 = 0 \\
& \Rightarrow C_1 = 0
\end{align}
\end{equation*}


\begin{equation*}
\begin{align}
& u(b) = C_2 = 0 \\
& \Rightarrow C_2 = 0
\end{align}
\end{equation*}

$\therefore$

\begin{equation*}
u = 0
\end{equation*}

$\mathcal{N}(L_{\tilde{m}})$ is trivial.

**** (b)

$f$ is a given function in $C[a,b]$.

To solve $L_{\tilde{m}}u = f$ for $u$,


\begin{equation*}
\begin{align}
-\frac{d^2u}{dx^2} & = f(x),\:a<x<b \\
\frac{du}{dx}(a) & = 0, \\
u(b) & = 0
\end{align}
\end{equation*}


\begin{equation*}
\begin{align}
\frac{d^2u}{dx^2}(x) & = -f(x) \\
\frac{du}{dx}(x) & = -\int\limits_a^xf(s)ds + C_1 \\
u(x) & = -\int\limits_a^x\int\limits_a^z f(s)dsdz + C_1x + C_2
\end{align}
\end{equation*}

The first boundary condition, $\frac{du}{dx}(a) = 0$:

\begin{equation*}
\begin{align}
\frac{du}{dx}(a) & = -\int\limits_a^a f(s)ds + C_1 = 0 \\
\int\limits_a^a f(s)ds & = 0 \\
& \Rightarrow C_1 = 0
\end{align}
\end{equation*}

The second boundary condition, $u(b) = 0$:


\begin{equation*}
\begin{align}
u(b) & = -\int\limits_a^b\int\limits_a^z f(s)dsdz + C_2 = 0 \\
& \Rightarrow C_2 = \int\limits_a^b\int\limits_a^z f(s)dsdz
\end{align}
\end{equation*}

$\therefore$


\begin{equation*}\begin{align}
u(x) & = -\int\limits_a^x\int\limits_a^z f(s)dsdz + \int\limits_a^b\int\limits_a^z f(s)dsdz \\
& = \int\limits_a^b\int\limits_a^z f(s)dsdz -\int\limits_a^x\int\limits_a^z f(s)dsdz \\
& = \int\limits_x^b\int\limits_a^z f(s)dsdz
\end{align}
\end{equation*}

\begin{equation*}
u(x) = \int\limits_x^b\int\limits_a^z f(s)dsdz
\end{equation*}

*** DONE 11.
CLOSED: [2025-08-19 Tue 20:04]

$D : C^1[a,b] \to C[a,b]$

\begin{equation*}
Df = \frac{df}{dx}
\end{equation*}

**** (a)

Show that the range of $D$ is all of $C[a,b]$.


\begin{equation*}
u(x) = \int_a^x f(s)ds
\end{equation*}

**** (b)

The solution is not unique. e.g.

\begin{equation*}
u(x) = \int_a^x f(s)ds + C
\end{equation*}

** DONE 3.3 [5/5]
*** DONE 1.
CLOSED: [2025-08-21 Thu 21:28]
**** (a)

\begin{equation*}
\mathbf{A} = \begin{bmatrix}\begin{array}{r r r}
3 & -1 & 2 \\ -1 & 3 & 4 \\ 2 & 0 & -3\end{array}\end{bmatrix},\:
\mathbf{x} = \begin{bmatrix}\begin{array}{r}
2 \\ -2 \\ 3
\end{array}\end{bmatrix}
\end{equation*}


\begin{equation*}\begin{align}
\mathbf{Ax} & = \begin{bmatrix}\begin{array}{r r r}
2(3) + (-2)(-1) + 3(2) \\
2(-1) + (-2)(3) + 3(4) \\
2(2) + -2(0) + 3(-3)
\end{array}\end{bmatrix} \\
& = \begin{bmatrix}\begin{array}{r r r}
6 + 2 + 6 \\
-2 + -6 + 12 \\
4 + 0 + -9
\end{array}\end{bmatrix} \\
& = \begin{bmatrix}\begin{array}{r r r}
14 \\
4 \\
-5
\end{array}\end{bmatrix}
\end{align}\end{equation*}

\begin{equation*}\begin{align}
2\begin{bmatrix}\begin{array}{r}3 \\ -1 \\ 2\end{array}\end{bmatrix} -
2\begin{bmatrix}\begin{array}{r}-1 \\ 3 \\ 0\end{array}\end{bmatrix} +
3\begin{bmatrix}\begin{array}{r}2 \\ 4 \\ -3\end{array}\end{bmatrix} & =
\begin{bmatrix}\begin{array}{r}6 \\ -2 \\ 4\end{array}\end{bmatrix} +
\begin{bmatrix}\begin{array}{r}2 \\ -6 \\ 0\end{array}\end{bmatrix} +
\begin{bmatrix}\begin{array}{r}6 \\ 12 \\ -9\end{array}\end{bmatrix} \\
& = \begin{bmatrix}\begin{array}{r}14 \\ 4 \\ -5\end{array}\end{bmatrix}
\end{align}\end{equation*}
**** (b)

$\mathbf{A} \in \mathbf{R}^{n\times n}$ and $\mathbf{x}\in\mathbf{R}^n$. The columns of $\mathbf{A}$ are vectors $\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n\in \mathbf{R}^n$ so that the $(i,j)$-entry of $\mathbf{A}$ is $(\mathbf{v}_j)_i$.

***** $(\mathbf{Ax})_i$

\begin{equation*}
(\mathbf{Ax})_i = \sum\limits_{j=1}^n x_j(\mathbf{v}_j)_i
\end{equation*}

***** $(x_1\mathbf{v}_1 + x_2\mathbf{v}_2+\dots+x_n\mathbf{v}_n)$

\begin{equation*}
(x_1\mathbf{x}_1 + x_2\mathbf{v}_2 + \dots + x_n\mathbf{v}_n) = \sum\limits_{j=1}^n x_j(\mathbf{v}_j)_i
\end{equation*}

*** DONE 3.
CLOSED: [2025-08-23 Sat 09:15]
Is

\begin{equation*}
\left\lbrace \begin{bmatrix}1 \\ 0 \\ 1\end{bmatrix}, \begin{bmatrix}1 \\ 1 \\ 1\end{bmatrix}, \begin{bmatrix}1 \\ 2 \\ 1\end{bmatrix} \right\rbrace
\end{equation*}

a basis for $\mathbf{R}^3$?

No. The first and third rows that make up the matrix of the three vectors are the same.

*** DONE 5.
CLOSED: [2025-08-23 Sat 09:46]

Typo in problem statement. The polynomials are independent. (See solution.pdf.)


\begin{equation*}
\lbrace 1 - x + 2x^2, 1 - 2x^2, 1 - 3x + 7x^2\rbrace
\end{equation*}

\begin{equation*}
\begin{align}
c_1(1-x+2x^2) c_2(1-x2^2) + c_3(1-3x+7x^2) = 0 \\
(c_1 + c_2 + c_3) + (-c_1 - 3c_3)x + (2c_1 - 2c_2 + 7c_3)x^2 = 0\\
\end{align}
\end{equation*}

\begin{equation*}
\begin{align}
c_1 + c_2 + c_2 & = 0 \\
-c_1 - 3c_3 & = 0 \\
2c_1 - 2c_2 + 7c_3 & = 0
\end{align}
\end{equation*}

\begin{equation*}
\mathbf{Ac} = 0
\end{equation*}

\begin{equation*}\mathbf{A} =
\begin{bmatrix}\begin{array}{r r r}
1 & 1 & 1 \\
-1 & 0 & -3 \\
2 & -2 & 7
\end{array}\end{bmatrix}
\end{equation*}


\begin{equation*}
\begin{bmatrix}\begin{array}{r r r}
1 & 1 & 1 \\ -1 & 0 & -3 \\ 2 & -2 & 7
\end{array}\end{bmatrix}\rightarrow
\begin{bmatrix}\begin{array}{r r r}
1 & 1 & 1 \\ 0 & 1 & -2 \\ 0 & -4 & 5
\end{array}\end{bmatrix}\rightarrow
\begin{bmatrix}\begin{array}{r r r}
1 & 1 & 1 \\ 0 & 1 & -2 \\ 0 & 0 & -3
\end{array}\end{bmatrix}
\end{equation*}

The row echelon form shows the system is non-singular and so it has only the trivial solution. The polynomials are independent.
*** DONE 7.
CLOSED: [2025-08-23 Sat 16:58]

Note: solution set gives a much better answer.

Show that $\lbrace L_1, L_2, L_3 \rbrace$ is a basis for $\mathcal{P}_2$


\begin{equation*}
\begin{align}
L_1(x) &= 2\left(x - \frac{1}{2}\right)(x-1) \\
L_2(x) &= -4x(x-1) \\
L_3(x) &= 2x\left(x-\frac{1}{2}\right)
\end{align}
\end{equation*}

For $x_1 = 0,\:x_2 = 1/2,\:x_3 = 1$,

\begin{equation*}
L_i(x_j) = \begin{cases} 1, & i = j \\ 0, & i \ne j.
\end{cases}
\end{equation*}

Let

\begin{equation*}
p(x) = p(x_1)L_1(x) + p(x_2)L_2(x) + p(x_3)L_3(x)
\end{equation*}

and $p(x) = c_1 + c_2x + c_3x^2$.

Therefore,

\begin{equation*}
\begin{align}
p(x_1) & = c_1, \\
p(x_2) & = c_1 + \frac{1}{2}c_2 + \frac{1}{4}c_3, \\
p(x_3) & = c_1 + c_2 + c_3.
\end{align}
\end{equation*}

\begin{equation*}
\begin{array}{l r r r r r r r}
p(x_1)L_1(x)  = & c_1 & - 3c_1x & + 2c_1x^2 & \\
p(x_2)L_2(x)  = & & 4c_1x & - 4c_1x^2 & + 2c_2x & - 2c_2x^2 & + c_3x & - c_3x^2 \\
p(x_3)L_3(x)  = & & -c_1x & + 2c_1x^2 & - c_2x & + 2c_2x^2 & - c_3x & + 2c_3x^2
\end{array}
\end{equation*}

Adding the above equations gives

\begin{equation*}
p(x_1)L_1(x) + p(x_2)L_2(x) + p(x_3)L_3(x) = c_1 + c_2x + c_3x^2
\end{equation*}.

$p(x)$ holds for every $p \in \mathcal{P}_2$. Therefore ${L_1, L_2, L_3}$ is the basis for $\mathcal{P}_2$.

*** DONE 9.
CLOSED: [2025-08-23 Sat 17:14]

Note: I think the solution in the manual is for problem 10.

Let

\begin{equation*}
L : C^2[a,b] \to C[a,b]
\end{equation*}

be the second derivative operator.

A basis for the null space of $L$ is

\begin{equation*}
\lbrace 1,x \rbrace
\end{equation*}

** TODO 3.4 [0/5]
*** TODO 1.
*** TODO 3.
*** TODO 5.
*** TODO 7.
*** TODO 9.
** TODO 3.5 [0/4]
*** TODO 1.
*** TODO 3.
*** TODO 5.
*** TODO 7.
